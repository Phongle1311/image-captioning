\documentclass[conference]{IEEEtran}
\IEEEoverridecommandlockouts
% The preceding line is only needed to identify funding in the first footnote. If that is unneeded, please comment it out.
\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}
\usepackage{hyperref}
\usepackage{multicol}
\usepackage{multirow}
\usepackage{orcidlink}
\usepackage{tabularx}
\usepackage{lscape}
\usepackage[utf8]{vietnam}
\usepackage{tikz,xcolor,hyperref}
\usepackage[normalem]{ulem}
\usepackage{tabularray}
\renewcommand{\arraystretch}{1.5}
\usepackage{etoolbox}
\makeatletter
\patchcmd{\@makecaption}
  {\scshape}
  {}
  {}
  {}
\makeatother
\hypersetup{
    colorlinks=true,
    linkcolor=blue,
    filecolor=magenta,      
    urlcolor=cyan,
    }
\definecolor{lime}{HTML}{A6CE39}
\DeclareRobustCommand{\orcidicon}{
	\begin{tikzpicture}
	\draw[lime, fill=lime] (0,0) 
	circle [radius=0.16] 
	node[white] {{\fontfamily{qag}\selectfont \tiny ID}};
	\draw[white, fill=white] (-0.0625,0.095) 
	circle [radius=0.007];
	\end{tikzpicture}
	\hspace{-2mm}
}
\foreach \x in {A, ..., Z}{\expandafter\xdef\csname orcid\x\endcsname{\noexpand\href{https://orcid.org/\csname orcidauthor\x\endcsname}
			{\noexpand\orcidicon}}
}
\newcommand{\orcidauthorA}{0009-0001-3187-4144}
\newcommand{\orcidauthorB}{0009-0004-3101-4840}

\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}

\renewcommand\thesection{\Roman{section}}
\renewcommand\thesubsection{\thesection.\arabic{subsection}/}
\begin{document}
\title{Sinh chú thích cho hình ảnh với cơ chế tập trung\\}
\author{\IEEEauthorblockN{Quang-Huy Ngyen\orcidA, Hoai-Phong Le\orcidB}
\IEEEauthorblockA{\textit{Falcuty of Information Technology, University of Science, VNU-HCM} \\
\textit{Vietnam National University, Ho Chi Minh City, Vietnam}\\
20120497@student.hcmus.edu.vn, 20120545@student.hcmus.edu.vn}
}
\maketitle

\begin{abstract}
Image captioning (chú thích hình ảnh tự động) là một bài toán đầy thử thách và có nhiều ứng dụng trong nhiều lĩnh vực khác nhau.
Tuy nhiên, mô hình encoder-decoder thông thường cho bài toán này có điểm yếu là chỉ sử dụng đặc trưng toàn cục của ảnh và có thể bỏ qua các chi tiết ngữ cảnh.
Bài nghiên cứu này sử dụng cơ chế Attention gắn với kiến trúc CNN-LSTM để tăng khả năng tập trung vào các phần quan trọng trong ảnh.
Kiến trúc này giúp câu mô tả chính xác hơn và tự nhiên hơn so với các mô hình trước đó.
Cơ chế trực quan hóa tập trung cũng giúp tăng khả năng giải thích của mô hình, từ đó có thể phát hiện được những dự đoán sai.
Chúng em huấn luyện mô hình trên hai tập dữ liệu tiêu chuẩn Flickr8k và Flickr30k. Mô hình được kiểm định bởi hai độ đo là BLEU và METEOR.
\end{abstract} 

\begin{IEEEkeywords}
Image Captioning, Deep Learning, Attention mechanism, Encoder-Decoder
\end{IEEEkeywords}

\section{Dẫn nhập}
Image captioning (chú thích ảnh / sinh mô tả cho ảnh) là một chủ đề thú vị và được được nghiên cứu rộng rãi trong lĩnh vực học máy.
Từ một tấm ảnh đầu vào, mô hình sẽ sinh ra một câu mô tả văn bản về nội dung bên trong bức ảnh.
Chủ đề này có nhiều ứng dụng trong thực tế, bao gồm: hỗ trợ cho người khiếm thị hiểu được nội dung của hình ảnh, cải thiện khả năng tìm kiếm và truy xuất hình ảnh, trợ lý cho các công việc liên quan đến xử lý hình ảnh, và ứng dụng trong nhiều lĩnh vực khác như truyền thông, quảng cáo, y tế ... 

Chỉ cần một cái liếc nhìn, con người có thể nhận ra và nêu được các thông tin chính trong bức ảnh\cite{fei2007we}.
Tuy nhiên đây là một nhiệm vụ đầy thử thách đối với học máy.
Nó yêu cầu mô hình không những phải đủ mạnh để giải quyết các thách thức về thị giác máy tính trong việc xác định được đối tượng nào nằm trong ảnh, mà còn phải có khả năng nắm bắt và thể hiện mối quan hệ của chúng bằng ngôn ngữ tự nhiên.

Bất chấp tính chất thách thức của bài toán này, gần đây đã có nhiều nghiên cứu liên quan đến việc giải quyết vấn đề tạo chú thích hình ảnh.
Một trong những phương pháp tiếp cận là sử dụng các đặc trưng ảnh được rút trích thủ công và dùng khuôn mẫu cố định để tạo ra câu mô tả. Ví dụ như các phương pháp được đề xuất bới Li et al.\cite{li2011composing} và Kuznetsova et al.\cite{kuznetsova2014treetalk}
Hạn chế của phương pháp này là  rút trích thông tin ảnh thủ công. Việc này rất tốn thời gian và công sức, đồng thời cũng gặp vấn đề như đặc trưng được rút trích không chính xác.

Nhờ sự hỗ trợ bởi những tiến bộ trong đào tạo mạng học sâu\cite{krizhevsky2017imagenet} và các tập dữ liệu kích thước lớn\cite{russakovsky2015imagenet}, chất lượng câu mô tả đã được cải thiện nhiều thông qua kiến trúc encoder - decoder (mã hóa - giải mã) với hai thành phần: Deep convolution neuron network (CNNs) để trích xuất đặc trưng hình ảnh và Recurrent newron network (RNNs) để mô hình hóa ngôn ngữ.

\begin{figure}[t]
\includegraphics[width=0.5\textwidth]{assets/example.jpg}
  \caption{Ví dụ chú thích cho hình ảnh dùng ngôn ngữ là tiếng Anh}
  \label{fig:example}
\end{figure}

Tại mỗi bước phát sinh ra từ kế tiếp của decoder, không cần đến thông tin của toàn bộ ảnh mà chỉ cần thông tin của vùng ảnh liên quan. Ý tưởng này được đề xuất bởi Xu et al.\cite{xu2015show}
 Cơ chế Attention cho phép ở mỗi bước phát sinh ra từ mới đều nhận được thông tin ảnh và chỉ tập trung vào thông tin của vùng ảnh cần thiết. Điều này giúp tăng độ chính xác của mô hình.
 Ngoài ra, cơ chế Attention còn giúp tăng khả năng giải thích của mô hình.
 Qua đó ta có thể hình dung được những gì mô hình đã học được bằng cách xem các vùng ảnh mà mô hình tập trung vào trong quá trình phát sinh ra câu mô tả.

Trong bài này, chúng em cài đặt mô hình sinh câu mô tả ảnh theo kiến trúc encoder-decoder với cơ chế soft-attention, trong đó sử dụng một mô hình CNN để làm encoder và RNN để làm decoder (phần \ref{method}).
Đồng thời, chúng em trình bày quá trình cài đặt và thực nghiệm để trích xuất kết quả từ hai tập dữ liệu Flickr8k\cite{hodosh2013framing} và Flickr30k\cite{young2014image} (phần \ref{implement}), kết quả được trình bày ở phần \ref{result}.

\section{Nghiên cứu liên quan}
\subsection{Giải pháp chung}
Những nghiên cứu về Image captioning hầu hết đều sử dụng mô hình học sâu với cấu trúc encoder - decoder khi chúng đã giải quyết vấn đề bằng cách kết hợp hai kĩ thuật về thị giác máy tính và xử lý ngôn ngữ tự nhiên.
Trong cấu trúc encoder - decoder, encoder sẽ trích xuất một vector chứa các feature bên trong ảnh đầu vào và decoder sẽ tạo ra một câu mô tả dựa trên vector chứa các feature tạo bởi encoder.
Encoder thường được dựa trên mô hình CNN và decoder thì dựa trên RNN. Các bài nghiên cứu đều dựa trên việc đề xuất mô hình hoặc cải tiến mô hình bằng các kĩ thuật khác nhau. 

\subsection{Một số kĩ thuật được đề xuất}
\subsubsection{CNNs và LSTMs} 
Mô hình LSTM (Long short-term memory) \cite{hochreiter1997long} là một phiên bản cải tiến của RNN và được dùng phổ biến hơn cho các toán liên quan đến thông tin dạng chuỗi khi nó giải quyết được vấn đề vanishing gradient trong việc xử lý. Ví dụ, Vinyals et al. đã đề xuất mô hình sử dụng CNN cho việc phân loại hình ảnh và LTSM cho việc tạo câu mô tả cho hình ảnh \cite{vinyals2015show} và đã có kết quả có độ chính xác tương đối ổn.
\subsubsection{R-CNNs và LSTM}
Mô hỉnh R-CNNs (Region-based CNNs) được giới thiệu bởi Girshick et al. \cite{girshick2014rich} dùng để giải quyết vấn đề nhận diện object với nhiều tính chất khác nhau (về mặt kích thước, hình dạng,...) khi mà mô hình CNN thông thường sử dụng grid với kích thước cố định. A.K.Yadav đã đề xuất mô hình sử dụng Fast R-CNNs tích hợp VGG-16 cho việc nhận diện vùng và trích xuất đặc trưng và sử dụng mô hình LSTM cho chú thích ảnh. Khuyết điểm của mô hình VGG là lượng tham số lớn và tính toán nặng.  
\subsubsection{ResNet-50 và LSTM}
Mô hình ResNet-50 là một mạng CNN có 50 lớp dùng để giải quyết hiện tượng Vanishing Gradient cũng như tránh tham số quá lớn giúp cho quá trình train diễn ra tốt và nhanh hơn.  Y. Chu \cite{chu2020automatic} đã đề xuất mô hình sử dụng ResNet-50 là một encoder để tạo vector một chiều biểu diễn đặc trưng trong ảnh và sử dụng mô hình LSTM là một decoder để decode vector tạo ra bởi encoder thành một câu chú thích. Đồng thời họ còn sử dụng soft attention ở decoder để có thể tập trung vào một số thành phần trong ảnh để dự đoán được câu tiếp theo tốt hơn. 
\subsubsection{Cơ chế Attention} 
Gần đây, mạng neuron sử dụng cơ chế attention được nghiên cứu, áp dụng cho tác vụ image captioning và tạo ra những đột phá bất ngờ.
Hình ảnh được chia thành các vùng lưới đều và thực hiện tính toán trên từng phần.
Khi RNNs sinh ra những từ mới, cơ chế này sẽ chỉ tập trung vào một số thành phần nhất định có liên quan.
Một ví dụ điển hình là mô hình được đề xuất trong \cite{yan2020image} cho hiệu quả cao hơn mô hình không dùng cơ chế tập trung.


\subsection{Các vấn đề mở rộng}
Có một số nghiên cứu gần đây tiếp cận image captioning theo hướng có thể giải thích được (explainable AI). Do mô hình "hộp đen" của học sâu, các phương pháp cũ không cung cấp đủ manh mối để giải quyết yêu cầu này. Seung-Ho Han et al. \cite{han2020explainable} đã đề xuất một số hướng để tạo một mô hình sinh chú thích ảnh có thể giải thích được.

Một hướng khác là tập trung vào vấn đề cá nhân hóa. Bộ sinh chú thích ảnh sẽ dựa thêm vào các thông tin được cung cấp trước đó về người sử dụng để tạo câu mô tả phù hợp với từng đối tượng người sử dụng khác nhau. Nghiên cứu \cite{chunseong2017attend} đề xuất mô hình Context Sequence Memory Networks để giải quyết vấn đề này.

\section{Phương pháp\label{method}}
\subsection{Tổng quan mô hình}
Mô hình theo kiến trúc Encoder - Decoder với cơ chế Attention. Trong đó:

a) Encoder: Là mạng neural tích chập (CNN). Có mục đích trích xuất đặc trưng của ảnh đầu vào. Đầu ra của mạng CNN là một vector đặc trưng có kích thước cố định.

b) Cơ chế Attention: Gán trọng số biểu thị mức độ quan trọng cho các đặc trưng hình ảnh. Mô hình sử dụng attention để tập trung vào các vùng quan trọng trong ảnh khi tiến hành tạo chú thích. Cụ thể, mô hình sử dụng cơ chế attention soft để định rõ sự tương quan giữa các đặc trưng của ảnh và từng từ trong chú thích.

c) Decoder: Là mạng neural hồi quy (LSTM). Đầu ra của mạng CNN được sử dụng làm đầu vào cho mạng RNN. Mạng LSTM sẽ sinh ra từng từ trong chú thích dựa trên đầu vào trước đó và các thông tin từ ảnh. 


\subsection{Encoder: CNNs}
Mô hình CNN là mạng neuron truyền thẳng (feedforward neural network), trong đó các lớp được liên kết với nhau bằng phép convolution. Trong đó phép convolution được định nghĩa theo công thức dưới:
$$conv(I,K)_{i,j}=\sum_{a}\sum_{b}\sum_{k}I_{i+a,j+b,k}K_{i,j,k}$$
Trong đó I là ảnh đầu vào nghãi là I là ma trận có kích thước ($n_{H},n_{W},n_{C})$, với $n_{H}$ là chiều cao ảnh, $n_{C}$ số channel, giả sử RGB thì sẽ có $n_{C}=3$, K là filter có kích thước ($n_{f},n_{f},n_{C})$.

\begin{figure}[h]
\includegraphics[width=0.5\textwidth]{assets/simpleCNN.jpeg}
  \caption{Minh họa mô hình CNN đơn giản}
  \label{fig:CNN_architecture}
\end{figure}

\begin{figure*}[t]
\centering
\includegraphics[width=0.85\textwidth]{assets/resnet.png}
  \caption{Kiến trúc của mô hình ResNet với số layer tương ứng }
  \label{fig:resnet_architecture}
\end{figure*}

Một lớp trong mạng CNN có thể là Convolutional layer, Pooling layer hoặc Fully connected layer. Convolutonal layer là layer chứa các filter tương tự như hidden units và output được tính bằng cách thực hiện phép convolution với đầu vào và sử dụng activation function. Pooling layer được sử dụng để giảm số chiều của ma trận, nhờ đó giảm số tham số để tính cho layer tiếp theo, có 2 loại thường sử dụng là lấy trung bình của một vùng (average) hoặc lấy giá trị lớn nhất trong một vùng (max) với kích thước cho trước. Fully connected layer là một layer chứa các neuron như mạng neuron nhân tạo truyền thống, nhận vào một vector và trả ra vector mới, vì vậy ma trận trước khi đưa vào Fully connected layer sẽ được trải phẳng ra trước, lúc đó vector sẽ có kích thước $n=n_{H}*n_{W}*n_{C}$. Hàm kích hoạt phổ biến nhất cho mạng CNN là hàm ReLU (rectified linear unit), có công thức là:
$$f(x)=max(0,x)$$
giúp cho việc tính toán nhanh hơn và tốc độ hội tụ nhanh hơn. Hình \ref{fig:CNN_architecture} thể hiện một mô hình CNN đơn giản.

Trong bài này, nhóm sử dụng một loại mô hình CNN có tên gọi là ResNet-50\cite{he2015deep}. Đây là một mô hình gồm 50 lớp và có thể giải quyết hiện tượng \textit{Vanishing Gradient} khi xây dựng mạng CNN bằng cách sử dụng các \textit{Residual block}, minh họa Residual block so với khối bình thường như ở hình \ref{fig:residual_block}. Với residual block, input có thể lan truyền xuôi (forward propagate) nhanh hơn thông qua cá residual connections (việc đưa input x vào phép tính tổng như hình \ref{fig:residual_block}) ở các layer. Hình \ref{fig:resnet_architecture} thể hiện chi tiết kiến trúc của mô hình ResNet với số layer tương ứng \cite{he2015deep}.

\begin{figure}[h]
\includegraphics[width=0.9\columnwidth]{assets/residual-block.png}
  \caption{Block bình thường (trái) và residual block (phải) }
  \label{fig:residual_block}
\end{figure}


Mô hình này sẽ tạo ra một tập hợp các vector thể hiện đặc trưng, cụ thể là L vector và với mỗi vector thuộc không gian $R^D$ thể hiện các phần trong ảnh. Để có thể trích xuất được vector đặc trưng từ layer, mô hình CNN sẽ bị bỏ bớt các Fully connected layer vì đây là những layer sử dụng để cho bài toán phân loại đối tượng. 


\subsection{Decoder: LSTMs}
Nhiệm vụ của bộ giải mã là dựa vào ảnh đã được mã hóa và sinh ra câu chú thích theo từng từ.
Các từ được sinh ra dựa vào các vector đặc trưng và cả những từ phía trước nó.
Mạng hồi quy (Recurrent Neural Network - RNN) là một dạng mạng nơ-ron phù hợp để xử lý dữ liệu chuỗi hoặc tuần tự.
Mạng RNN có khả năng xử lý thông tin từ quá khứ và sử dụng nó để tạo ra dự đoán cho các đầu ra trong tương lai.
Điều này rất hữu ích trong việc mô hình hóa các chuỗi dữ liệu có tính tuần tự như ngôn ngữ tự nhiên.

\begin{figure}[h]
\includegraphics[width=0.5\textwidth]{assets/RNN_model.png}
  \caption{Minh họa kiến trúc RNN}
  \label{fig:RNN_architecture}
\end{figure}

Tuy nhiên RNN gặp vấn đề khi lưu trữ thông tin dài hạn.
Điều này gây khó khăn trong việc huấn luyện mạng và ảnh hưởng đến khả năng của mô hình trong việc ghi nhớ thông tin quan trọng.
Trong quá trình lan truyền ngược mạng RNN, đạo hàm có thể bị bùng nổ hoặc tiêu biến (exploding/vanishing gradient).
Do đó, trong nghiên cứu này, chúng tôi sử dụng biến thể mạnh mẽ của RNN là LSTM\cite{hochreiter1997long} (Long Short-term Memory).

Một đơn vị LSTM có cấu trúc phức tạp hơn đơn vị RNN.'
Nó sử dụng các cổng kiểm soát luồng thông tin (gọi là cổng quên - forgot gate).
Do dó, LSTM có khả năng ghi nhớ thông tin một cách chọn lọc.
LSTM có khả năng ghi nhớ thông tin dài hạn, có khả năng học được các phụ thuộc xa.

Công thức cập nhật trạng thái (cell state) tại thời điểm t trong một đơn vị LSTM được biểu diễn như sau:
$$
\begin{pmatrix}
i \\
f \\
o \\
g
\end{pmatrix}
= 
\begin{pmatrix}
\sigma \\
\sigma \\
\sigma \\
tanh
\end{pmatrix}
\circ
W
\begin{pmatrix}
h_{t-1} \\
x
\end{pmatrix}
$$
$$
c_t = f\circ c_{t-1} + i\circ g
$$
$$
h_t = o \circ tanh(c_t)
$$
$$
p_t = softmax(h_t)
$$

Trong đó, $\sigma$ là hàm sigmoid, i, f, o, g là các cổng quên, với $p_t$ là phân phối xác suất với tất cả các từ. Từ có xác suất lớn nhất được chọn ở từng bước và sẽ được đưa vào bước tiếp theo để tạo thành câu. 

\begin{figure}[h]
\includegraphics[width=0.5\textwidth]{assets/RNN_LSTM_compare.png}
  \caption{So sánh kiến trúc của đơn vị RNN với đơn vị LSTM}
  \label{fig:RNN_LSTM_compare}
\end{figure}

Trong bài toán image captioning, sử dụng LSTM làm bộ giải mã là lựa chọn phổ biến và hiệu quả.
LSTM cho phép mô hình học các mẫu ngôn ngữ phức tạp, xử lý thông tin từ ảnh và tạo ra các từ dự đoán phù hợp.
Đồng thời, LSTM cũng giúp mô hình duy trì trạng thái ẩn và ghi nhớ thông tin quan trọng từ các từ đã sinh ra trước đó, tạo nên sự liên kết và logic trong câu chú thích.

\begin{figure}[h]
\includegraphics[width=0.5\textwidth]{assets/architecture_CNN_LSTM.png}
  \caption{Minh họa mô hình CNN - LSTM}
  \label{fig:CNN_LSTM}
\end{figure}

\begin{figure}[h]
\includegraphics[width=0.5\textwidth]{assets/architecture_CNN_LSTM_attention.png}
  \caption{Minh họa mô hình CNN - LSTM - Attention}
  \label{fig:CNN_LSTM_Attention}
\end{figure}

\subsection{Cơ chế attention}
\begin{figure*}[t]
\centering
\includegraphics[width=\textwidth]{assets/attention.png}
  \caption{Minh họa vùng ảnh mà mô hình tập trung vào trong quá trình phát sinh câu mô tả. Những vùng ảnh được quan tâm sẽ sáng hơn phần còn lại.}
  \label{fig:RNN_LSTM_attention}
\end{figure*}

Hạn chế của mô hình CNN-LSTM hiện tại là thông tin ảnh chỉ được đưa một lần duy nhất tại bước đầu tiên của LSTM, nghĩa là cố gắng decode cả ảnh từ tằng cuối $h_o$.
Ta có thể giải quyết bằng cách đưa toàn bộ thông tin ảnh vào từng bước.
Tuy nhiên, phương pháp này lại gây tốn kém chi phí tính toán.

Một giải pháp tốt hơn là chỉ đưa vào LSTM các phần quan trọng trong ảnh. Để làm điều này, chúng ta có thể sử dụng cơ chế attention mềm (soft attention). Đây là một cơ chế được sử dụng rộng rãi  
để giải quyết các vấn đề về phân loại ảnh. Soft attention được sử dụng bằng cách thêm cổng attention vào LSTM để có thể gán trọng số theo mức độ quan trọng như hình \ref{fig:CNN_LSTM_Attention}. Soft attention phụ thuộc vào kết quả của LTSM ở bước trước đó và đặc trưng đầu vào.  Khi đó, LSTM có công thức cập nhật mới là
$$
\begin{pmatrix}
i \\f \\o \\g \\\alpha_t
\end{pmatrix}
= 
\begin{pmatrix}
\sigma \\ \sigma \\ \sigma \\ tanh \\ softmax
\end{pmatrix}
\circ
W
\begin{pmatrix}
I \circ \alpha_{t-1} \\ h_{t-1} \\ x
\end{pmatrix}
$$
Trong đó: $\alpha_t$ là tham số attention tại thời điểm t. Vói việc sử dụng softmax thì luôn đảm bảo $\alpha_t>0$ ở mỗi pixel và với vector chứa tất cả pixel thì ta có $\sum{}{}\alpha_t=1$ Soft attention là hảm khả vi nên ta có thể sử dụng gradient descent kết hợp backpropagation để huấn luyện. 


\section{Thực nghiệm}
\subsection{Tập dữ liệu}
Chúng em thực hiện thực nghiệm và đánh giá mô hình trên hai tập dữ liệu phổ biến là Flickr8k\cite{hodosh2013framing} và Flickr30k\cite{young2014image}.
Mỗi tập lần lượt bao gồm 8,000 và 30,000 ảnh khác nhau, mỗi ảnh đi kèm với 5 câu chú thích bằng tiếng Anh được mô tả bởi con người.
Các hình ảnh được chọn từ sáu nhóm Flickr khác nhau, có xu hướng không chứa bất kỳ người hoặc địa điểm nổi tiếng nào, và được chọn thủ công để mô tả nhiều cảnh vật và tình huống khác nhau.
\begin{table}[!h]
\centering
\caption{Bảng số liệu về cách chia số ảnh cho các tập huấn luyện, tập kiểm thử và tập kiểm tra trên hai bộ dữ liệu Flickr8k, Flickr30k}
\label{table:dataset-split}
\begin{tabularx}{\columnwidth} { 
  | >{\centering\arraybackslash}X 
  | >{\centering\arraybackslash}X 
  | >{\centering\arraybackslash}X 
  | >{\centering\arraybackslash}X | }
 \hline
\textbf{Dataset name} & \textbf{Train} & \textbf{Validate} & \textbf{Test} \\ \hline
Flickr8k  & 6,000  & 1,000    & 1,000 \\ \hline
Flickr30k & 29,000 & 1,000    & 1,000 \\ \hline
\end{tabularx}
\end{table}

\begin{figure*}[h]
\includegraphics[width=\textwidth]{assets/predict.pdf}
  \caption{Một số ví dụ về dự đoán đúng và chưa đúng của mô hình được huấn luyện trên tập Flickr30k. Hai ảnh phía trên bên trái cho câu mô tả đúng, hai ảnh phía trên bên phải cho câu mô tả sai một phần (mô tả đúng chủ thể chính trong ảnh), hai ảnh phía dưới cho câu mô tả sai hoàn toàn. }
  \label{fig:predict}
\end{figure*}

\begin{table*}[h]
\centering
\caption{Bảng so sánh kết quả đánh giá độ đo BLEU-1,2,3,4 và METEOR của mô hình tự cài đặt lại so với những mô hình đã được công khai khác. (---) biểu thị giá trị độ đo chưa biết.}
\label{tab:metric-compare-1}
\resizebox{0.75\textwidth}{!}{%
\begin{tabular}{ccccccc}
\cline{3-6}
\multicolumn{1}{l}{} & \multicolumn{1}{l|}{} & \multicolumn{4}{c|}{\textbf{BLEU}} & \multicolumn{1}{l}{} \\ \hline
\multicolumn{1}{|c|}{\textbf{Tập dữ liệu}} & \multicolumn{1}{c|}{\textbf{Mô hình}} & \multicolumn{1}{c|}{\textbf{BLEU-1}} & \multicolumn{1}{c|}{\textbf{BLEU-2}} & \multicolumn{1}{c|}{\textbf{BLEU-3}} & \multicolumn{1}{c|}{\textbf{BLEU-4}} & \multicolumn{1}{c|}{\textbf{METEOR}} \\ \hline
\multicolumn{1}{|c|}{\multirow{4}{*}{Flickr8k}} & Mao et al.\cite{mao2014explain} & 0.580 & 0.280 & 0.230 & --- & \multicolumn{1}{c|}{---} \\
\multicolumn{1}{|c|}{} & Google NIC\cite{vinyals2015show} & 0.630 & 0.410 & 0.270 & --- & \multicolumn{1}{c|}{---} \\
\multicolumn{1}{|c|}{} & Log Bilinear\cite{kiros2014unifying} & \textbf{0.656} & 0.424 & 0.277 & 0.177 & \multicolumn{1}{c|}{0.173} \\
\multicolumn{1}{|c|}{} & CNN-LSTM-Attention & 0.647 & \textbf{0.469} & \textbf{0.333} & \textbf{0.232} & \multicolumn{1}{c|}{\textbf{0.209}} \\ \hline
\multicolumn{1}{|c|}{\multirow{4}{*}{Flickr30k}} & Mao et al. & 0.550 & 0.240 & 0.200 & --- & \multicolumn{1}{c|}{---} \\
\multicolumn{1}{|c|}{} & Google NIC & 0.663 & 0.423 & 0.277 & 0.183 & \multicolumn{1}{c|}{---} \\
\multicolumn{1}{|c|}{} & Log Bilinear & 0.600 & 0.380 & 0.254 & 0.171 & \multicolumn{1}{c|}{0.169} \\
\multicolumn{1}{|c|}{} & CNN-LSTM-Attention & \textbf{0.665} & \textbf{0.481} & \textbf{0.344} & \textbf{0.244} & \multicolumn{1}{c|}{\textbf{0.195}} \\ \hline
\end{tabular}%
}
\end{table*}

Chúng em chia các tập huấn luyện, tập kiểm thử và tập kiểm tra bằng cách chia\footnote{\url{http://cs.stanford.edu/people/karpathy/deepimagesent/}} của Karpathy \& Li \cite{karpathy2015deep}.
Số lượng mỗi ảnh trong mỗi tập được thể hiện trong bảng \ref{table:dataset-split}


\subsection{Độ đo đánh giá}
Việc đánh giá câu mô tả cho hình ảnh không phải là việc dễ dàng. Đối với các bài toán phân lớp, nhận diện thông thường, ta chỉ việc tính tỉ lệ được phân lớp đúng hay nhận diện đúng. Còn đối với bài toán sinh mô tả cho ảnh, ngoài việc đánh giá nội dung câu chú thích có chính xác hay không, ta cần phải đánh giá tính mạch lạc, chính xác về mặt ngữ pháp. Để đánh giá chất lượng câu mô tả được phát sinh, chúng em sử dụng hai độ đo đánh giá là BLEU\cite{papineni2002bleu} và METEOR\cite{banerjee2005meteor}.

Độ đo BLEU (BiLingual Evaluation Understudy) là độ đo được sử dụng phổ biến trong bài toán dịch máy.
Độ đo này tính tỉ lệ n-gram so khớp đúng giữa câu ứng viên và các câu tham chiếu. 

Độ đo BLEU tồn tại một số hạn chế.
Nó chỉ thực sự tốt khi đánh giá ở mức toàn bộ ngữ liệu, nhưng đối với bài toán mô tả ảnh, thì độ đo này không thực sự tốt.
Do đó, chúng em sử dụng thêm độ đo METEOR  (Metric for Evaluation of Translation with Explicit ORdering).
METEOR không giới hạn n-gram mà sẽ lấy những chuỗi dài nhất có thể để đánh giá độ bao phủ.
METEOR cũng mở rộng tìm kiếm so khớp với các từ đồng nghĩa giúp đa dạng hóa hơn trong việc đánh giá so khớp.
Thực nghiệm cho thấy METEOR có mức độ tương quan với sự đánh giá của con người trong bài toán phát sinh mô tả ảnh tốt hơn so với BLEU.

Điểm số các độ đo này nằm trong khoảng từ 0 đến 1, trong đó 1 đại diện cho sự tương đồng hoàn toàn giữa câu ứng viên và các câu tham chiếu.

\subsection{Quá trình thử nghiệm\label{implement}}
Giai đoạn tiền xử lý thì bao gồm xây dựng bộ từ vựng, xử lý các câu mô tả và ảnh đầu vào. Bộ từ vựng được xây dựng bằng tất cả từ khác nhau trong các câu mô tả đầu vào và dựa vào tần suất xuất hiện của từ để xếp vị trí. Các token <start>, <end>, <unk>, <pad> được thêm vào bộ từ vựng và được sử dụng để tiền xử lý câu mô tả. Các câu mô tả được tiền xử lý bằng cách thêm <start> ở đầu câu và <end> ở cuối câu, thay thế các từ có tần suất xuất hiện nhỏ nhỏ hơn 5 lần bằng <unk> và thêm token <pad> nếu như số từ trong câu ít hơn số câu tối đa để các câu mô tả có kích thước cố định. Sau đó các câu mô tả được encode các từ theo vị trí trong bộ từ vựng. 
Các ảnh sẽ được giảm kích thước xuống (256, 256) đồng thời cắt chính giữa kích thước (224, 224) và normalize ảnh bằng mean and standard deviation theo tiêu chuẩn của các ảnh ImageNet được sử dụng trong các pre-trained model của Pytorch\footnote{\label{pytorch}\url{https://pytorch.org/vision/main/models.html}}.  

Chúng em sử dụng mô hình ResNet-50 làm encoder để trích xuất đặc trưng. Mô hình ResNet-50 được nhóm sử dụng là mô hình ResNet-50 đã được huấn luyện 1.2 triệu ảnh trên tập ImageNet\cite{russakovsky2015imagenet}, mô hình được cung cấp bởi thư viện Torchvision\footref{pytorch}. Vì vậy có thể tái sử dụng mô hình này và bỏ bớt hai lớp cuối (được sử dụng cho bài toán phát hiện đối tượng) để trích xuất đặc trưng cho ảnh đầu vào. Với mỗi ảnh đầu vào, ta sẽ thu được được feature map kích thước 14x14x2048. Sau đó vector này được làm input để đưa qua decoder xử lý.

Decoder sẽ xử lý trên feature map tạo bởi encoder được trải phẳng thành kích thước 196x2048. Với mỗi bước decode, vector feature map và trạng thái ẩn trước đó $h_{t-1}$ để tính trọng số attention cho từng pixel $\alpha_{t}$, sau đó từ vựng được tạo từ bước trước đó kết hợp với trọng số attention để sinh ra từ tiếp theo. Sau đó sử dụng một lớp Linear để chuyển output từ decoder sang score cho từng từ trong bộ từ vựng. Beam search với kích thước bằng 5 cũng được sử dụng tương tự như trong \cite{xu2015show} để sinh ra câu mô tả tốt hơn. 

Thuật toán Adam \cite{kingma2014adam} được sử dụng cho cả encoder và decoder trong quá trình huấn luyện để tối ưu hóa quá trình huấn luyện. Tương tự trong \cite{xu2015show}, chúng em cũng sử dụng kĩ thuật train theo mini-batch để giúp tăng tốc độ hội tụ, với mỗi epoch thì thuật toán sẽ chia dữ liệu thành các mini-batch với số lượng nhất định và các hệ số sẽ được cập nhật với mỗi lần duyệt một mini-batch. Chúng em cũng sử dụng kĩ thuật learning rate scheduler để có thể giảm hệ số học khi điểm BLEU-4 không cải thiện sau một số epoch nhất định. 

Với mỗi tập dữ liệu Flickr8k và Flickr30k thì việc huấn luyện sẽ chia làm hai giai đoạn: giai đoạn đầu thì sẽ không fine-tune encoder mà chỉ huấn luyện và cập nhật hệ số decoder với mini-batch có kích thước là 64 trong khoảng tầm 15-20 epoch cho đến khi mà điểm BLEU trong tập validation không tăng nữa sau số epoch nhất định. Giai đoạn thứ hai thì sẽ fine-tune encoder với mini-batch là 32 khi nhận thấy việc fine-tune encoder có thể giúp cải thiện điểm BLEU một chút và dừng sau số epoch nhất định khi điểm BLEU không tăng nữa, ở cả hai tập dữ liệu thì chúng em dừng sau khoảng 5 epoch. 

\subsection{Kết quả đánh giá\label{result}}
\subsubsection{Phân tích định tính}
Chúng em đưa ra ví dụ dự đoán từ mô hình được huấn luyện trên tập dữ liệu Flickr30k ở hình \ref{fig:predict}. Dựa vào cơ chế tập trung, mô hình có thể nắm bắt và xác định được các chủ thể chính trong ảnh và đưa ra dự đoán khớp với trực quan của con người.

Ở một vài trường hợp, mô hình đưa ra dự đoán đúng về chủ thể chính nhưng lại không đúng ở chủ thể khác xung quanh. Đối với một số mẫu phức tạp, có khả năng gây nhầm lẫn cao khiến cho mô hình đưa ra dự đoán sai. Tuy nhiên, nhờ vào cơ chế trực quan hóa những vùng mà mô hình tập trung vào để đưa ra quyết định (hình \ref{fig:RNN_LSTM_attention}), chúng em có thể hình dung được một cách trực quan lý do gây ra những sai lầm trong dự đoán của mô hình.

\subsubsection{Phân tích định lượng}
Bảng \ref{tab:metric-compare-1} cung cấp tóm tắt kết quả đánh giá của mô hình so với một số mô hình tiêu biểu khác trên hai tập dữ liệu Flickr8k và Flickr30k thông qua hai độ đo BLEU và METEOR.
Ta thấy câu mô tả được phát sinh bởi mô hình CNN-LSTM-Attention có kết quả tốt hơn so với các mô hình trước đó.
Thông tin ảnh đã được bổ sung nhiều hơn khi sử dụng cơ chế tập trung.

\section{Kết luận}


\renewcommand{\refname}{Tài liệu tham khảo}
\bibliographystyle{plain}
\bibliography{refs}

\end{document}
